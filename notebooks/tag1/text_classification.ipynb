{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.resources import files\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "\n",
    "from aml_wa24 import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = load_dataset(\"sst2\", split=\"train\").to_polars()\n",
    "\n",
    "eval_size = 100\n",
    "train_size = 1000\n",
    "assert eval_size + train_size <= len(sst2) # ensure that the training data does not contain evaluation data\n",
    "\n",
    "sst2 = sst2.sample(fraction=1, shuffle=True, seed=42) # shuffle df\n",
    "eval_df = sst2.head(eval_size)\n",
    "train_df = sst2.tail(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = str(files(models).joinpath(\"paraphrase-MiniLM-L3-v2\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Task: Understand what is going on here.\n",
    "def in_batches(df: pl.DataFrame, shuffle: bool = False):\n",
    "    if shuffle:\n",
    "        df = df.sample(fraction=1, shuffle=True)\n",
    "    batches = df.with_row_index(\"batch\").with_columns((pl.col(\"batch\") / batch_size).floor()).partition_by(\"batch\", include_key=False)\n",
    "    return [(b[\"sentence\"].to_list(), b[\"label\"].to_torch()) for b in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df: pl.DataFrame):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for texts, label in in_batches(df):\n",
    "            labels.append(label)\n",
    "            features = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "            preds.append(model(**features)[\"logits\"])\n",
    "        preds = torch.cat(preds)\n",
    "        labels = torch.cat(labels)\n",
    "        loss = loss_fn(preds, labels).item()\n",
    "        accuracy = multiclass_accuracy(preds, labels).item()\n",
    "        return {\"loss\": loss, \"acc\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for texts, labels in in_batches(train_df, shuffle=True):\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        features = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "        preds = model(**features)[\"logits\"]\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(evaluate(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# Write a method that takes a string and predicts the sentiment with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Task (torch)\n",
    "# Try to execute the Notebook on colab.\n",
    "# Try to use a gpu.\n",
    "\n",
    "# Tip: you have to activate a gpu first, by default colab only uses cpu\n",
    "# Tip: You can check if the model is running on gpu with\n",
    "print(model.device) # \"cpu\" = cpu; \"cuda\" = gpu\n",
    "# Tip: You can move the model to another device with\n",
    "model = model.to(\"cpu\") # \"cuda\" for gpu\n",
    "# Tip: torch.tensors (and other torch components) work exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Task\n",
    "# Try out a larger model for example \"google-bert/bert-base-uncased\"\n",
    "# You can find other models at https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Task\n",
    "# What is better? basic_text_classififcation.ipynb or this notebook? Why? What are the differences?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
